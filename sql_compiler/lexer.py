#SQLç¼–è¯‘å™¨ - è¯æ³•åˆ†æå™¨

# sql_compiler/lexer.py
import re
from typing import List, Tuple, Optional
from utils.constants import TokenType, KEYWORDS, OPERATORS, DELIMITERS


class Token:
    def __init__(self, token_type: TokenType, lexeme: str, line: int, column: int):
        self.token_type = token_type
        self.lexeme = lexeme
        self.line = line
        self.column = column

    def __repr__(self):
        return f"[{self.token_type.name}, '{self.lexeme}', {self.line}, {self.column}]"


class Lexer:
    def __init__(self):
        self.tokens = []
        self.line = 1
        self.column = 1
        self.current_pos = 0
        self.input = ""

    def tokenize(self, input_text: str) -> List[Token]:
        self.input = input_text
        self.tokens = []
        self.line = 1
        self.column = 1
        self.current_pos = 0

        while self.current_pos < len(self.input):
            char = self.input[self.current_pos]

            # è·³è¿‡ç©ºç™½å­—ç¬¦
            if char.isspace():
                if char == '\n':
                    self.line += 1
                    self.column = 1
                else:
                    self.column += 1
                self.current_pos += 1
                continue

            # å¤„ç†æ³¨é‡Š
            if char == '-' and self.current_pos + 1 < len(self.input) and self.input[self.current_pos + 1] == '-':
                self.skip_comment()
                continue

            # å¤„ç†å­—ç¬¦ä¸²å¸¸é‡
            if char == "'":
                token = self.process_string()
                if token:
                    self.tokens.append(token)
                continue

            # å¤„ç†æ•°å­—å¸¸é‡
            if char.isdigit():
                token = self.process_number()
                if token:
                    self.tokens.append(token)
                continue

            # å¤„ç†æ ‡è¯†ç¬¦å’Œå…³é”®å­—
            if char.isalpha() or char == '_':
                token = self.process_identifier()
                if token:
                    self.tokens.append(token)
                continue

            # å¤„ç†è¿ç®—ç¬¦
            if char in OPERATORS:
                token = self.process_operator()
                if token:
                    self.tokens.append(token)
                continue

            # å¤„ç†åˆ†éš”ç¬¦
            if char in DELIMITERS:
                self.tokens.append(Token(TokenType.DELIMITER, char, self.line, self.column))
                self.column += 1
                self.current_pos += 1
                continue

            # æœªçŸ¥å­—ç¬¦
            raise Exception(f"Lexical error: Unknown character '{char}' at line {self.line}, column {self.column}")

        return self.tokens

    def skip_comment(self):
        while (self.current_pos < len(self.input) and
               self.input[self.current_pos] != '\n'):
            self.current_pos += 1
            self.column += 1

    def process_string(self) -> Optional[Token]:
        start_pos = self.current_pos
        start_line = self.line
        start_column = self.column

        self.current_pos += 1  # è·³è¿‡å¼€å§‹çš„å¼•å·
        self.column += 1

        string_content = []
        while (self.current_pos < len(self.input) and
               self.input[self.current_pos] != "'"):
            if self.input[self.current_pos] == '\n':
                self.line += 1
                self.column = 1
            else:
                self.column += 1
            string_content.append(self.input[self.current_pos])
            self.current_pos += 1

        if self.current_pos >= len(self.input):
            raise Exception(f"Lexical error: Unclosed string at line {start_line}, column {start_column}")

        # è·³è¿‡ç»“æŸçš„å¼•å·
        self.current_pos += 1
        self.column += 1

        lexeme = "'" + ''.join(string_content) + "'"
        return Token(TokenType.CONSTANT, lexeme, start_line, start_column)

    def process_number(self) -> Optional[Token]:
        start_pos = self.current_pos
        start_line = self.line
        start_column = self.column

        number_content = []
        while (self.current_pos < len(self.input) and
               (self.input[self.current_pos].isdigit() or self.input[self.current_pos] == '.')):
            number_content.append(self.input[self.current_pos])
            self.current_pos += 1
            self.column += 1

        lexeme = ''.join(number_content)
        return Token(TokenType.CONSTANT, lexeme, start_line, start_column)

    def process_identifier(self) -> Optional[Token]:
        start_pos = self.current_pos
        start_line = self.line
        start_column = self.column

        identifier_content = []
        while (self.current_pos < len(self.input) and
               (self.input[self.current_pos].isalnum() or self.input[self.current_pos] == '_')):
            identifier_content.append(self.input[self.current_pos])
            self.current_pos += 1
            self.column += 1

        lexeme = ''.join(identifier_content)
        # ğŸ‘‡ å…³é”®ä¿®å¤ï¼šå°† lexeme è½¬æ¢ä¸ºå¤§å†™åå†ä¸ KEYWORDS é›†åˆæ¯”è¾ƒ
        if lexeme.upper() in KEYWORDS:
            return Token(TokenType.KEYWORD, lexeme.upper(), start_line, start_column)
        else:
            return Token(TokenType.IDENTIFIER, lexeme, start_line, start_column)

    def process_operator(self) -> Optional[Token]:
        start_pos = self.current_pos
        start_line = self.line
        start_column = self.column

        operator_content = [self.input[self.current_pos]]
        self.current_pos += 1
        self.column += 1

        # å¤„ç†å¤šå­—ç¬¦è¿ç®—ç¬¦
        if (self.current_pos < len(self.input) and
                operator_content[0] + self.input[self.current_pos] in OPERATORS):
            operator_content.append(self.input[self.current_pos])
            self.current_pos += 1
            self.column += 1

        lexeme = ''.join(operator_content)
        return Token(TokenType.OPERATOR, lexeme, start_line, start_column)